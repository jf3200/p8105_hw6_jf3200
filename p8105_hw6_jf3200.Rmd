---
title: "Homework 6"
author: "Jessica Flynn"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load_package_and_settings}
library(tidyverse)
library(p8105.datasets)
library(modelr)
library(gtsummary)

set.seed(1)


theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
  )
```


## Problem 1 

```{r import_and_clean_homicide_data}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate( 
    city_state = str_c(city, state, sep = "_"),
    victim_age = as.numeric(victim_age),
    resolved = case_when(
      disposition == "Closed without arrest" ~ 0, 
      disposition =="Open/No arrest" ~ 0, 
      disposition == "Closed by arrest" ~ 1)) %>%
  filter(victim_race %in% c("White", "Black"),
         city_state != "Tulsa_AL", 
         city_state != "Dallas_TX", 
         city_state != "Phoenix_AZ", 
         city_state != "Kansas City_MO") %>%
  select(city_state, resolved, victim_age, victim_race, victim_sex)
```

Start with one city.

```{r baltimore_example}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "Baltimore_MD")


baltimore_model = 
  glm(resolved ~ victim_age + victim_race + victim_sex,
    data = baltimore_df, 
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate), 
    CI_lower = exp(estimate - 1.96*std.error), 
    CI_upper = exp(estimate + 1.96*std.error)) %>% 
  select(term, OR, starts_with("CI"))

baltimore_model %>% 
  knitr::kable(digits = 3)

```

The  estimate of the adjusted odds ratio for solving homicides comparing white victims to black victims keeping all other variables fixed is `r round(baltimore_model %>% filter(term == "victim_raceWhite") %>% pull(OR),3)` and the confidence interval is (`r round(baltimore_model %>% filter(term == "victim_raceWhite") %>% pull(CI_lower),3)`, `r round(baltimore_model %>% filter(term == "victim_raceWhite") %>% pull(CI_upper), 3)`). This means that white victims have significantly higher odds of their homicides being solved compared to black victims in Baltimore. 

Try this across cities.

```{r map_across_cities}
model_results_df =
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(models = map(.x = data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())), 
         results = map(models, broom::tidy)) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate), 
    CI_lower = exp(estimate - 1.96*std.error), 
    CI_upper = exp(estimate + 1.96*std.error)) %>% 
  select(city_state, term, OR, starts_with("CI")) %>% 
  print()
```

```{r estimate_plot}
model_results_df %>% 
  filter(term == "victim_raceWhite") %>%
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x =  element_text(angle = 90, vjust = 0.5, hjust = 1)) + 
  labs(x = "City and State", 
       y = "Odds Ratio", 
       title = "Odds Ratios for Solving Homicides in White vs. Black Victims")
```

This plot shows that for most cities, the odds ratio appears to be above 1. An odds ratio above 1 means that the odds of solving homicides for white victims is higher compared to solving homicides for black victims, although it may not be significantly different depending on the confidence intervals. In Boston, MA, this OR is the largest with the odds of a crime being solved for a white victim at over 10 times the odds of it being solves for a black victim.

## Problem 2

```{r import_clean_birthweight_data}
birth_df = 
  read_csv("data/birthweight.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(babysex = case_when(babysex == 1 ~ "male", 
                             babysex == 2 ~ "female"), 
         malform = case_when(malform == 0 ~ "absent", 
                             malform == 1 ~ "present")) %>% 
  mutate_at(
    vars(contains("race")), 
    funs(case_when(
      . == 1 ~ "white", 
      . == 2 ~"black", 
      . == 3 ~ "asian",
      . == 4 ~ "puerto rican", 
      . == 8 ~ "other", 
      . == 9 ~ "unknown")))

```

Next, we will build a model to predict `bwt`, the child's birthweight. The first step in this modeling process will be to look at which variables alone are significant predictors of birthweight. This is referred to as univariable analysis. We will conduct this for all variables and use `bwt` as the outcome.

```{r uva}
tbl_uvregression(birth_df, 
                 method = lm, 
                 y = bwt)

```

From the univariable analysis, we see that many variables are significant predictors of bwt. A few of these variables are likely to be highly correlated. For example, `ppbmi` (mother's pre-pregnancy BMI) and `ppwt` (mother's pre-pregnancy weight) and are similar measures. Additionally, `delwt`(mother's weight at delivery) is likely highly correlated with `ppbmi`, as is `mheight`(mother's height). `delwt` is also likely correlated with `ppwt` and `wtgain`(mother's weight gain during pregnancy).   

Since `ppbmi` has height and weight information all in one, we will use only this and not all of the potentially correlated variables in our multivariable model(`ppwt`, `delwt`, `mheight`, `wtgain`). Corrlated variables can lead to multicollinearity issues in models, which we want to avoid.  

`pnumlbw` and `pnumsga` are both columns of all 0 values, so no beta was able to be estimated, thus, they will not be included in further modeling.  

Since only Black vs Asian is significant for both `mrace` and `frace` and all levels are not significant, we will not include these variables in the multivariable model.

Now, we will put all of the remaining significant, uncorrelated variables into a multivariable model. 

```{r mva}
model_multi = lm(bwt ~ babysex + bhead + blength + fincome  + gaweeks  + momage  + ppbmi + smoken, data = birth_df)

model_multi %>% 
  broom::tidy() %>% 
  knitr::kable()
```


All of the variables in our model are independent significant predictors of a baby's birthweight. Now, we will plot of fitted values vs residuals. Below, we see that the majority of the residuals hover around 0, and that there is no obvious pattern in the residuals. However, there are some extreme residuals which may be a concern. 

```{r fitted_resid_plot}
birth_df %>% 
  add_residuals(model_multi) %>% 
  add_predictions(model_multi) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  labs(x = "Fitted Values", 
       y = "Residuals", 
       title = "Residuals vs. Fitted Values Plot")
```

Next, we will do a model comparison using `crossv_mc` for cross validation. We will compare our constructed model to a model using length at birth and gestational age as predictors, and to a model using head circumference, length, sex, and all interactions between these. 

```{r  compare_plots}
model_main = lm(bwt ~ blength + gaweeks, data = birth_df)
model_interaction = lm(bwt ~ bhead * blength * babysex, data = birth_df)

cv_df = 
  crossv_mc(birth_df, 100) 

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(
    model_main  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_interaction = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x)),
    model_multi  = map(train, ~lm(bwt~ babysex + bhead + blength + fincome  + gaweeks  + momage  + ppbmi + smoken, data = .x))) %>% 
  mutate(
    rmse_main = map2_dbl(model_main, test, ~rmse(model = .x, data = .y)),
    rmse_interaction = map2_dbl(model_interaction, test, ~rmse(model = .x, data = .y)),
    rmse_multi = map2_dbl(model_multi, test, ~rmse(model = .x, data = .y)))

```


We will plot the RMSE values from the 3 models and compare them. 

```{r plot_rsme}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() + 
  labs(x = "Model", 
       y = "RMSE", 
       title = "Comparison of RMSE Values by Model")
```

Looking at the RSME plot, we see that the model with the lowest RMSE is the model we created from our univariable/multivariable analysis approach. Despite the RMSE being the lowest, it may not necessarily be a better model than the model using head circumference, length, sex, and all interactions. We have to think carefully about which model to choose -- the interaction model is more complicated to interpret, but our model has more predictors. 

## Problem 3

For this problem, we will import 2017 Central Park weather data

```{r import_weather_df}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


We will create the 5,000 bootstrap samples from the weather data 

```{r}
boot_straps = 
  weather_df %>% 
  modelr::bootstrap(n = 5000)
```

Now, we will run a linear model on each bootstrapped sample and extract the relevant information we need (r-squared and log(beta0 * beta1))

```{r analyze_bootstraps, cache=TRUE}
bootstrap_results = 
  boot_straps %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance), 
    results2 = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(c(results, results2), names_repair = "universal") %>% 
  select(.id, term, estimate, r.squared) %>% 
  pivot_wider(id_cols = c(.id, r.squared),
              names_from = term,
              values_from = estimate) %>% 
  rename(b0 = `(Intercept)`, 
         b1 = "tmin") %>% 
  mutate(logb0b1 = log(b0 * b1)) 

```

Looking at the plots below, we see that both r-squared and log(b0*b1) are both normally distributed. The means and 95% confidence intervals for each quantity are also provided.

```{r plot_distributions}
bootstrap_results %>% 
  ggplot(aes(x = r.squared)) + 
  geom_histogram() +
  labs(x = "R-squared",
       y = "Count",
       title = "Distribution of R-squared")


bootstrap_results %>% 
  ggplot(aes(x = logb0b1)) + 
  geom_histogram() +
  labs(x = "log(beta0 * beta1)",
       y = "Count",
       title = "Distribution of log(beta0 * beta1)")


```

```{r sample_cis}
bootstrap_results %>% 
  select(-b0, -b1) %>% 
  pivot_longer(r.squared:logb0b1, 
               names_to = "term", 
               values_to = "estimate") %>%
  group_by(term) %>% 
  summarize(
    mean = mean(estimate),
    ci_lower = quantile(estimate, 0.025), 
    ci_upper = quantile(estimate, 0.975)) %>% 
  knitr::kable(digits = 3)


```
